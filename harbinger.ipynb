{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Implementation of MobileNetv2+SSD<br>\nThis is an implementation of the MobileNetv2 + SSD architecture for a relatively simpler task of determining bounding boxes for MNIST images embedded in a box. Each box contains only one digit(28x28 MNIST embedded into a 224x224 box) as of now, but the number of predictions per image can be expanded easily (the training outputs need to modified). Also, no data augmentation has been used till now (Colab kept crashing when I increased the dataset size beyond 1000, so the initial amount of data present was sufficient. The crashes might have been due to high traffic, but I haven't confirmed it).<p>\nIn the earlier implementation, the ground truth data contained information about only one bounding box, which meant only one prediction per image (reference in README). For me, it also reduced the training signal and the model was overfitting. So I changed the outputs to a prediction for each default box (as it should be, from what I understood from the SSD paper). Although the initial implementation is good for the purposes for understanding the model.\n\nComments mentioned throughout the code mention what needs to change if the model inputs or outputs are changed.","metadata":{"id":"wbsreEfjnQtP"}},{"cell_type":"markdown","source":"Import libraries","metadata":{"id":"TdL2AL8yAi00"}},{"cell_type":"code","source":"import os\nimport numpy as np\nimport numpy.matlib\nfrom PIL import Image\nimport cv2 \nimport matplotlib.pyplot as plt","metadata":{"id":"_hvSY3X-AeFP","execution":{"iopub.status.busy":"2023-07-24T15:04:26.080222Z","iopub.execute_input":"2023-07-24T15:04:26.081322Z","iopub.status.idle":"2023-07-24T15:04:26.331435Z","shell.execute_reply.started":"2023-07-24T15:04:26.081264Z","shell.execute_reply":"2023-07-24T15:04:26.329993Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# TRAIN = False\n# SAVE = False\n# LOAD = True\n\nTRAIN = True\nSAVE = True\nLOAD = False","metadata":{"execution":{"iopub.status.busy":"2023-07-24T15:04:26.334613Z","iopub.execute_input":"2023-07-24T15:04:26.335451Z","iopub.status.idle":"2023-07-24T15:04:26.340608Z","shell.execute_reply.started":"2023-07-24T15:04:26.335398Z","shell.execute_reply":"2023-07-24T15:04:26.339529Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Using other model","metadata":{}},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nfrom torchvision import datasets\nfrom torchvision import transforms as T\nimport torchvision.transforms.functional as F\nfrom torch.utils.data import ConcatDataset,DataLoader\nfrom torchvision.models import densenet201\n\nSIZE = 30","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2023-07-24T15:04:26.572452Z","iopub.execute_input":"2023-07-24T15:04:26.573280Z","iopub.status.idle":"2023-07-24T15:04:30.641913Z","shell.execute_reply.started":"2023-07-24T15:04:26.573236Z","shell.execute_reply":"2023-07-24T15:04:30.640624Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model=densenet201(pretrained=True)\nclassifier_in=model.classifier.in_features\nmodel.classifier=nn.Linear(classifier_in,10)","metadata":{"execution":{"iopub.status.busy":"2023-07-24T15:04:30.644380Z","iopub.execute_input":"2023-07-24T15:04:30.645173Z","iopub.status.idle":"2023-07-24T15:04:37.152385Z","shell.execute_reply.started":"2023-07-24T15:04:30.645128Z","shell.execute_reply":"2023-07-24T15:04:37.151068Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class ScaledResizePad(object):\n    def __init__(self, output_size=SIZE, scale_size=18, fill=0,padding_mode='constant'):\n        self.fsize = output_size\n        self.dsize = scale_size\n\n        self.fill = 0\n        self.padding_mode = padding_mode\n#         assert isinstance(min_size, (numbers.Number, str, tuple))\n#         assert isinstance(max_size, (numbers.Number, str, tuple))\n        \n    def __call__(self, img):\n        \"\"\"\n        Args:\n            img (PIL Image): Image to be padded.\n\n        Returns:\n            PIL Image: Padded image.\n        \"\"\"\n        if type(img) == torch.Tensor:\n            w,h = img.shape[1:]\n        elif type(img) == np.ndarray:\n            w,h = img.shape[:2]\n        else:\n            w,h = img.size\n        scale_factor = self.dsize/max(w,h)\n        size = int(h*scale_factor),int(w*scale_factor)\n        \n        scaled_img = F.resize(img,size)\n#         scaled_img = F.resize(img,(20,20))\n        \n        return F.pad(scaled_img, get_padding(scaled_img,self.fsize), self.fill, self.padding_mode)\n    \n    def __repr__(self):\n        return self.__class__.__name__ + '(padding={0}, fill={1}, padding_mode={2})'.\\\n            format(self.fill, self.padding_mode)","metadata":{"execution":{"iopub.status.busy":"2023-07-24T15:04:37.154472Z","iopub.execute_input":"2023-07-24T15:04:37.154909Z","iopub.status.idle":"2023-07-24T15:04:37.166532Z","shell.execute_reply.started":"2023-07-24T15:04:37.154865Z","shell.execute_reply":"2023-07-24T15:04:37.165232Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_padding(image,output_size = SIZE):\n    if type(image) == torch.Tensor or type(image) == np.ndarray:\n        w, h = image.shape[1:]\n    else:\n        w, h = image.size \n    h_padding = (output_size - w) / 2\n    v_padding = (output_size - h) / 2\n    l_pad = h_padding if h_padding % 1 == 0 else h_padding+0.5\n    t_pad = v_padding if v_padding % 1 == 0 else v_padding+0.5\n    r_pad = h_padding if h_padding % 1 == 0 else h_padding-0.5\n    b_pad = v_padding if v_padding % 1 == 0 else v_padding-0.5\n    padding = (int(l_pad), int(t_pad), int(r_pad), int(b_pad))\n    return padding\n\nclass NewPad(object):\n    def __init__(self, fill=0, padding_mode='constant'):\n#         assert isinstance(fill, (numbers.Number, str, tuple))\n        assert padding_mode in ['constant', 'edge', 'reflect', 'symmetric']\n\n        self.fill = fill\n        self.padding_mode = padding_mode\n        \n    def __call__(self, img):\n        \"\"\"\n        Args:\n            img (PIL Image): Image to be padded.\n\n        Returns:\n            PIL Image: Padded image.\n        \"\"\"\n        return F.pad(img, get_padding(img), self.fill, self.padding_mode)\n    \n    def __repr__(self):\n        return self.__class__.__name__ + '(padding={0}, fill={1}, padding_mode={2})'.\\\n            format(self.fill, self.padding_mode)\n\n\nclass RandomResize(object):\n    def __init__(self, min_size,max_size):\n        self.min_size = min_size\n        self.max_size = max_size\n#         assert isinstance(min_size, (numbers.Number, str, tuple))\n#         assert isinstance(max_size, (numbers.Number, str, tuple))\n        \n    def __call__(self, img):\n        \"\"\"\n        Args:\n            img (PIL Image): Image to be padded.\n\n        Returns:\n            PIL Image: Padded image.\n        \"\"\"\n        r1 = torch.rand(1)[0].item()\n        r2 = 1-torch.rand(1)[0].item()*0.2+0.1\n        w = int(self.min_size + (self.max_size - self.min_size)*r1)\n        h = int(w*r2)\n        return F.resize(img,(w,h))\n    \n    def __repr__(self):\n        return self.__class__.__name__ + '(padding={0}, fill={1}, padding_mode={2})'.\\\n            format(self.fill, self.padding_mode)\n\n    \nt =  T.Compose([\n        T.RandomRotation(35),\n        RandomResize(15,30),\n        NewPad(),\n        T.ToTensor()\n    ])","metadata":{"execution":{"iopub.status.busy":"2023-07-24T15:04:37.168265Z","iopub.execute_input":"2023-07-24T15:04:37.168638Z","iopub.status.idle":"2023-07-24T15:04:37.186427Z","shell.execute_reply.started":"2023-07-24T15:04:37.168608Z","shell.execute_reply":"2023-07-24T15:04:37.185154Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"device = 'cuda' if torch.cuda.is_available() else 'cpu'","metadata":{"execution":{"iopub.status.busy":"2023-07-24T15:04:55.872320Z","iopub.execute_input":"2023-07-24T15:04:55.872769Z","iopub.status.idle":"2023-07-24T15:04:55.880817Z","shell.execute_reply.started":"2023-07-24T15:04:55.872735Z","shell.execute_reply":"2023-07-24T15:04:55.878875Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if TRAIN:\n    batch=500\n    training_data=datasets.MNIST(root='/kaggle/input/mobilenet',train=True,download=True,transform=t)\n    test_data=datasets.MNIST(root='/kaggle/input/mobilenet',train=False,download=True,transform=t)\n    training_data=ConcatDataset([training_data,test_data])\n    train_loader=DataLoader(training_data,batch_size=batch)\n\n    criterion=nn.CrossEntropyLoss()\n    optimiser=torch.optim.Adam(model.parameters(),lr=1e-4)\n    \n    model.to(device)\n    for epoch in range(5):\n        epoch_loss=0\n        model.train()\n        for index,sample in enumerate(train_loader):\n            if index %500 == 0:\n                print(f\"Epoch {epoch}, index {index}\")\n            image,label=sample\n            image=image.repeat(1,3,1,1)\n            image=image.to(device)\n            label=label.to(device)\n            optimiser.zero_grad()\n\n            output=model(image)        \n            loss=criterion(output,label)\n            loss.backward()\n            optimiser.step()\n\n            epoch_loss+=loss.item()     \n        print('Epoch:{} Training Loss:{}'.format(epoch+1,epoch_loss))\n    model.eval()\n","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2023-07-24T15:04:37.188450Z","iopub.execute_input":"2023-07-24T15:04:37.188767Z","iopub.status.idle":"2023-07-24T15:04:37.209015Z","shell.execute_reply.started":"2023-07-24T15:04:37.188741Z","shell.execute_reply":"2023-07-24T15:04:37.207866Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if SAVE:\n    torch.save(model.state_dict(),'MNIST.pth')\nif LOAD:\n    weights=torch.load('../input/densenet/MNIST.pth',map_location=device)\n    model.load_state_dict(weights)\n    model.eval()\n    print('Weights Loaded Successfully')","metadata":{"execution":{"iopub.status.busy":"2023-07-24T15:04:57.923645Z","iopub.execute_input":"2023-07-24T15:04:57.924121Z","iopub.status.idle":"2023-07-24T15:04:58.312771Z","shell.execute_reply.started":"2023-07-24T15:04:57.924083Z","shell.execute_reply":"2023-07-24T15:04:58.311042Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Testing","metadata":{}},{"cell_type":"code","source":"img_path = \"/kaggle/input/temp-dataset-pos/machine3_randomised.png\"","metadata":{"execution":{"iopub.status.busy":"2023-07-24T14:58:25.121543Z","iopub.execute_input":"2023-07-24T14:58:25.121978Z","iopub.status.idle":"2023-07-24T14:58:25.126498Z","shell.execute_reply.started":"2023-07-24T14:58:25.121942Z","shell.execute_reply":"2023-07-24T14:58:25.125295Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"full_img = cv2.imread(img_path)\n\nprint(full_img.shape)\n# plt.show(img[900:1600,700:1340,:]) # 1\nimg = full_img[350:700,:,:] # 2&3\n# img = full_img[600:,:400,:] # 2&3\nplt.imshow(img)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-07-24T14:58:26.732902Z","iopub.execute_input":"2023-07-24T14:58:26.734057Z","iopub.status.idle":"2023-07-24T14:58:27.049691Z","shell.execute_reply.started":"2023-07-24T14:58:26.733998Z","shell.execute_reply":"2023-07-24T14:58:27.048823Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"img = full_img[300:700,:,:].copy()\n# img = cv2.resize(img,(224, 224))\nresult = np.zeros_like(img)\n\ndef sort_contours(cnts):\n    boundingBoxes=[cv2.boundingRect(contour) for contour in contours]\n    (cnts, boundingboxes) = zip(*sorted(zip(cnts, boundingBoxes),key=lambda b:b[1][0]))\n    return cnts,boundingboxes\n\ngray=cv2.cvtColor(img,cv2.COLOR_RGB2GRAY)\nmedian = numpy.median(gray)\nedged = cv2.Canny(img, int(0.6 * median), int(1 * median))\n# edged=cv2.Canny(gray,100,200)\ncontours,heirarchy = cv2.findContours(edged.copy(), cv2.RETR_EXTERNAL,cv2.CHAIN_APPROX_SIMPLE)\ncontours,boundingboxes=sort_contours(contours)\n\nlabel = 0\nfor i,box in enumerate(boundingboxes):\n    x,y,w,h=box   \n    if (w>=5 and w <= 50 and h>=20 and h <= 100 and h/w<5 and w/h<5):\n        label += 1\n#         img = cv2.rectangle(img, (x, y), (x+w, y+h), (255, 0, 0), 2) #Plotting bounding box\n#         img = cv2.putText(img, str(label), (x, y-3), cv2.FONT_HERSHEY_SIMPLEX, 0.3, 1) #Plotting texts on top of box\n#         cv2.drawContours(img, [contours[i]], -1, (255, 255, 255), 3)#3\n#         # bounding box\n#         result = cv2.rectangle(result, (x, y), (x+w, y+h), (255, 0, 0), 2) #Plotting bounding box\n# #         # label\n#         result = cv2.putText(result, str(label), (x, y-3), cv2.FONT_HERSHEY_SIMPLEX, 1,(255, 0, 0), 1) #Plotting texts on top of box\n        # draw white filled contour on black background\n        cv2.drawContours(result, [contours[i]], 0, (255,255,255), 3) #cv2.FILLED\n\n\n# plt.imshow(img)\nplt.imshow(result)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-07-24T14:58:31.061676Z","iopub.execute_input":"2023-07-24T14:58:31.062407Z","iopub.status.idle":"2023-07-24T14:58:31.349881Z","shell.execute_reply.started":"2023-07-24T14:58:31.062368Z","shell.execute_reply":"2023-07-24T14:58:31.348850Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def predict(image):\n    img=cv2.cvtColor(image, cv2.COLOR_RGB2GRAY)\n    transform=T.Compose([T.ToPILImage(),ScaledResizePad(padding_mode='edge',fill=0),T.ToTensor()])  \n    img=1-transform(img)\n    img=img.repeat(1,3,1,1).to(device)\n    output=model(img)\n    score,predicted=torch.max(output,1)\n    return score.item(),predicted.item(), output\n\ndigits=[]\nscores = []\nfor box in boundingboxes:\n    x,y,w,h=box   \n    if (w>=5 and w <= 50 and h>=20 and h <= 100 and h/w<5 and w/h<5):\n#         print(h,w,w/h,h/w)\n        number=img[y-3:y+h+3,x-3:x+w+3]\n        score, digit, raw = predict(number)\n        \n        if score < 1:\n            continue\n        if digit == 7:\n            print(digit, raw,h/w)\n        if digit == 7 and h/w > 2:\n            if raw[0][1].item()>1:\n                digit = 1\n                score = raw[0][1].item()\n        predict_img = cv2.rectangle(result, (x, y), (x+w, y+h), (255, 0, 0), 2) #Plotting bounding box\n        # label\n        predict_img = cv2.putText(predict_img, str(digit), (x, y-3), cv2.FONT_HERSHEY_SIMPLEX, 1,(255, 0, 0), 1) #Plotting texts on top of box\n\n        digits.append(digit)\n        scores.append(score)\nprint(digits)\nprint(scores)\n\nplt.imshow(predict_img)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-07-24T15:02:10.914111Z","iopub.execute_input":"2023-07-24T15:02:10.914496Z","iopub.status.idle":"2023-07-24T15:02:11.612112Z","shell.execute_reply.started":"2023-07-24T15:02:10.914465Z","shell.execute_reply":"2023-07-24T15:02:11.610045Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}